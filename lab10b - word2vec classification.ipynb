{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab10b.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dorienh/computational_data_science/blob/master/lab10b%20-%20word2vec%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HAA2XCLSQInJ"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification with word2vec \n",
        "\n",
        "-- Prof. Dorien Herremans\n",
        "\n",
        "We will be tackling a classification problem by first creating word embeddings, and comparing this to alternative approaches. \n",
        "\n",
        "During this tutorial, you will need some of the following libraries: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "aCtMQEnRQjhV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install bs4 \n",
        "# !pip install sklearn\n",
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install lxml\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import random\n",
        "import gensim\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "368EVM3_QInK"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the dataset\n",
        "\n",
        "The classification problem at hand is to predict the tag that belongs to a stack overflow post. \n",
        "\n",
        "The data from Google BigQuery is publicly available at this Cloud Storage URL:\n",
        "\n",
        "https://storage.googleapis.com/tensorflow-workshop-examples/stack-overflow-data.csv.\n",
        "\n",
        "Like before, I first load my dataset from my private dropbox to Google Colab. Those of you working on your own machines, no need for this step. Skip to the next one. \n"
      ]
    },
    {
      "metadata": {
        "id": "dBLM0vEjyarI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4dfad7cf-cff7-4c77-ca2f-8bd476ef914b"
      },
      "cell_type": "code",
      "source": [
        "# !pip install dropbox\n",
        "import dropbox\n",
        "access_token = 'your_token_get_at' # https://www.dropbox.com/developers/apps\n",
        "dbx = dropbox.Dropbox(access_token)\n",
        "\n",
        "j = \"/path_within_your_dropbox/stack-overflow-data.csv\"\n",
        "dbx.files_download_to_file('/stack-overflow-data.csv', j)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileMetadata(name='stack-overflow-data.csv', id='id:F3kUIwrgEV8AAAAAAAtYjg', client_modified=datetime.datetime(2018, 10, 17, 9, 0, 11), server_modified=datetime.datetime(2018, 10, 22, 10, 7, 19), rev='6fbcab5bbb0', size=44319561, path_lower='/cds/week 10 - word2vec/lab/data/stack-overflow-data.csv', path_display='/CDS/week 10 - word2vec/lab/data/stack-overflow-data.csv', parent_shared_folder_id='3400907696', media_info=None, symlink_info=None, sharing_info=FileSharingInfo(read_only=False, parent_shared_folder_id='3400907696', modified_by='dbid:AACoui9q33n9Ww_K5AZehsrUdiK8zsU0_1E'), property_groups=None, has_explicit_shared_members=None, content_hash='c482a2aa50f49ec41b402f8efc522bd35f5f97f575318f91c1fc142573019f62')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "JMzTr6hCQInM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start by having a look at our data: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3758TYmIQInQ",
        "outputId": "eb584c23-f283-4507-c311-75d6eb3b0246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "cell_type": "code",
      "source": [
        "!mv /stack-overflow-data.csv /data\n",
        "df = pd.read_csv('data/stack-overflow-data.csv', encoding = 'latin-1')\n",
        "\n",
        "# only keep data that has a tag: \n",
        "df = df[pd.notnull(df['tags'])]\n",
        "df.head(10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>what is causing this behavior  in our c# datet...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>have dynamic html load as if it was in an ifra...</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>how to convert a float value in to min:sec  i ...</td>\n",
              "      <td>objective-c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>.net framework 4 redistributable  just wonderi...</td>\n",
              "      <td>.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>trying to calculate and print the mean and its...</td>\n",
              "      <td>python</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>how to give alias name for my website  i have ...</td>\n",
              "      <td>asp.net</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>window.open() returns null in angularjs  it wo...</td>\n",
              "      <td>angularjs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>identifying server timeout quickly in iphone  ...</td>\n",
              "      <td>iphone</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>unknown method key  error in rails 2.3.8 unit ...</td>\n",
              "      <td>ruby-on-rails</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>from the include  how to show and hide the con...</td>\n",
              "      <td>angularjs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                post           tags\n",
              "0  what is causing this behavior  in our c# datet...             c#\n",
              "1  have dynamic html load as if it was in an ifra...        asp.net\n",
              "2  how to convert a float value in to min:sec  i ...    objective-c\n",
              "3  .net framework 4 redistributable  just wonderi...           .net\n",
              "4  trying to calculate and print the mean and its...         python\n",
              "5  how to give alias name for my website  i have ...        asp.net\n",
              "6  window.open() returns null in angularjs  it wo...      angularjs\n",
              "7  identifying server timeout quickly in iphone  ...         iphone\n",
              "8  unknown method key  error in rails 2.3.8 unit ...  ruby-on-rails\n",
              "9  from the include  how to show and hide the con...      angularjs"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "13ECXgs-xn9p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The size of our model will depend on how many unqiue words are in the dataset (meaning in the article text or posts): "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "BvAXbtE5QInW",
        "outputId": "8c90857f-cf8c-4640-877a-829bac7da688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Count the number of words: \n",
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10286120"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LSifacVqQIna"
      },
      "cell_type": "markdown",
      "source": [
        "We have over 10 million words in the data. That's a lot! \n",
        "\n",
        "\n",
        "Let's define a set of tags that we will be particularly interested in to judge our classification results later, and visulaise our dataset: \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "O-DdZVX9QInb",
        "outputId": "908ca675-a946-41ad-8f32-d26752318ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "cell_type": "code",
      "source": [
        "# defining our tags\n",
        "my_tags = ['java','html','asp.net','c#','ruby-on-rails','jquery','mysql','php','ios','javascript','python','c','css','android','iphone','sql','objective-c','c++','angularjs','.net']\n",
        "\n",
        "# visualising dataset\n",
        "plt.figure(figsize=(10,4))\n",
        "df.tags.value_counts().plot(kind='bar');"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlkAAAEmCAYAAABRZIXOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npa8bLeHOOgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fb6840e7198>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "5UwHhJXMQIng"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the classes are very well balanced.\n",
        "\n",
        "Now let's have a look at the post data in more detail: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "2Q3m4GxxQInh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a32d5fc-5ba3-45b5-abe5-c30510409208"
      },
      "cell_type": "code",
      "source": [
        "print(df['post'].values[1])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dynamic html load iframe aspnet 40 site users save entire html page backend database want load dynamic content div existing page content area couple things happen want css affect anything outside div first trying loading badly formed html would move images divs outside content area around lot html pages use base tag images links want base tag respected inside div solution going try use iframe set url another child page loads dynamic html page entirely wondering better solution\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "nbSGI0FQQInu"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see, the text needs to be cleaned up a bit. Below we use the nltk toolkit to remove spaces, stopwords, symbols etc. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bydjpKCBQInv",
        "outputId": "9e0d1fab-ccf7-44ee-ae04-7a3858e9a37e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# note: slower students may wish to skip this step to finish the lab in class\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "# !pip install lxml\n",
        "import lxml\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "#     text = BeautifulSoup(text, features=\"xml\").text\n",
        "    text = BeautifulSoup(text, 'html.parser').text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6A8nlP0hQInx",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['post'] = df['post'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "MlL2uGKsQIn0",
        "outputId": "bf91bb46-62c7-4407-f900-ead1c62ce1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(df['post'].values[1])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dynamic html load iframe aspnet 40 site users save entire html page backend database want load dynamic content div existing page content area couple things happen want css affect anything outside div first trying loading badly formed html would move images divs outside content area around lot html pages use base tag images links want base tag respected inside div solution going try use iframe set url another child page loads dynamic html page entirely wondering better solution\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "YTkqUfwzQIn8"
      },
      "cell_type": "markdown",
      "source": [
        "This looks a lot better!\n",
        "\n",
        "Now how many unique words do we have in this cleaned up dataset? "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4oV5baXxQIn8",
        "outputId": "60d44a56-4f2b-4e3c-c98a-af70f2458a5a",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "df['post'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3424194"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gpllc3QyQIoA"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have over 3 million words to work with.\n",
        "\n",
        "Before we start creating some classifiers, let's split our dataset in a test set (for evaluation) and training set: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ylA7e4H_QIoB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df.post\n",
        "y = df.tags\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "cDHptCZqQIoU"
      },
      "cell_type": "markdown",
      "source": [
        "### Logistic regression\n",
        "\n",
        "Now that we have our features, we can train a classifier to try to predict the tag of a post. We will start with logistic, which provides a nice baseline for this task. \n",
        "\n",
        "To make the vectorizer => transformer => classifier easier to work with, we will use Pipeline class in Scilkit-Learn that behaves like a compound classifier."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "p8IMmMZWQIoV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "5d8042e3-0287-4d41-9df2-5903033ef71d"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "logreg = Pipeline([('vect', CountVectorizer()),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
        "               ])\n",
        "logreg.fit(X_train, y_train)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "        strip...ty='l2', random_state=None,\n",
              "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ZGSVTzWRQIoY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "c0460bb7-58f5-4bd4-db9e-e90361f6c4a1"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.7819166666666667\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.70      0.62      0.66       613\n",
            "         html       0.91      0.91      0.91       620\n",
            "      asp.net       0.97      0.94      0.95       587\n",
            "           c#       0.78      0.77      0.78       586\n",
            "ruby-on-rails       0.77      0.81      0.79       599\n",
            "       jquery       0.59      0.58      0.58       589\n",
            "        mysql       0.77      0.75      0.76       594\n",
            "          php       0.81      0.86      0.83       610\n",
            "          ios       0.69      0.71      0.70       617\n",
            "   javascript       0.61      0.59      0.60       587\n",
            "       python       0.64      0.64      0.64       611\n",
            "            c       0.82      0.83      0.83       594\n",
            "          css       0.78      0.78      0.78       619\n",
            "      android       0.84      0.85      0.84       574\n",
            "       iphone       0.80      0.83      0.81       584\n",
            "          sql       0.65      0.64      0.65       578\n",
            "  objective-c       0.82      0.84      0.83       591\n",
            "          c++       0.91      0.91      0.91       608\n",
            "    angularjs       0.96      0.94      0.95       638\n",
            "         .net       0.78      0.83      0.80       601\n",
            "\n",
            "  avg / total       0.78      0.78      0.78     12000\n",
            "\n",
            "CPU times: user 1.04 s, sys: 9.03 ms, total: 1.05 s\n",
            "Wall time: 1.05 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zAAVJw44xn_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "That's quite a good accuracy. Now let's see if we can combine word2vec with logistic regression by feeding the new embedded representation to our logistic regression instead of the bag of words. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9gSX1ysMQIoc"
      },
      "cell_type": "markdown",
      "source": [
        "### Word2vec embedding and Logistic Regression"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ubl-sOB8W2f1"
      },
      "cell_type": "markdown",
      "source": [
        "Let's load a pretrained word2vec model, and use the embedding representation as input to a simple classifier (i.e. logistic regression). \n",
        "\n",
        "You can use the word2vec model you trained in lab 10a, or load this (quite big, 1.5GB) pretrained word2vec model: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n",
        "\n",
        "Note: it can take a while to load. (takes 2min for me) \n",
        "\n",
        "Alternatively, load from Dropbox to Colab again (skip this is you are not doing this):"
      ]
    },
    {
      "metadata": {
        "id": "KgU9iEDo0h7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "e3a784a6-6e20-4b7a-c02a-87a327999ac6"
      },
      "cell_type": "code",
      "source": [
        "# !pip install dropbox\n",
        "import dropbox\n",
        "access_token = 'your_token' # https://www.dropbox.com/developers/apps\n",
        "dbx = dropbox.Dropbox(access_token)\n",
        "\n",
        "j = \"/yourpathindropbox/GoogleNews-vectors-negative300.bin.gz\"\n",
        "dbx.files_download_to_file('/GoogleNews-vectors-negative300.bin.gz', j)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileMetadata(name='GoogleNews-vectors-negative300.bin.gz', id='id:F3kUIwrgEV8AAAAAAAtYbA', client_modified=datetime.datetime(2018, 10, 17, 9, 32, 40), server_modified=datetime.datetime(2018, 10, 22, 9, 24, 46), rev='69bcab5bbb0', size=1647046227, path_lower='/cds/week 10 - word2vec/lab/data/googlenews-vectors-negative300.bin.gz', path_display='/CDS/week 10 - word2vec/lab/data/GoogleNews-vectors-negative300.bin.gz', parent_shared_folder_id='3400907696', media_info=None, symlink_info=None, sharing_info=FileSharingInfo(read_only=False, parent_shared_folder_id='3400907696', modified_by='dbid:AACoui9q33n9Ww_K5AZehsrUdiK8zsU0_1E'), property_groups=None, has_explicit_shared_members=None, content_hash='75c1b5da05d0b4662d0631aba2dda7a7d9268a85da7bf057ecefbd905e21de1d')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "-pM8IWFy0jDH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the file is on your system: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Cjipngb9QIod",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "wv = gensim.models.KeyedVectors.load_word2vec_format(\"data/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "wv.init_sims(replace=True)\n",
        "print('Model loaded')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E-W7lH8wxn_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you are interested how good the embeddings are, you could try some of the similarity tests we did in Lab 10a. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mC4XwSuzQIoo"
      },
      "cell_type": "markdown",
      "source": [
        "As we have multiple words for each post, we will need to somehow combine them, a common way is to average the\n",
        "word vectors. This is a BOW approach (typically includes averaging, summation, weighted addition)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lbSLtiwyQIoo",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def word_averaging(wv, words):\n",
        "    # averages a set of words 'words' given their wordvectors 'wv'\n",
        "    \n",
        "    all_words, mean = set(), []\n",
        "    \n",
        "    for word in words:\n",
        "        # if the words are alread vectors, then just append them\n",
        "        if isinstance(word, np.ndarray):\n",
        "            mean.append(word)\n",
        "        # if not: first get the vector embedding for the words\n",
        "        elif word in wv.vocab:\n",
        "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
        "            all_words.add(wv.vocab[word].index)\n",
        "\n",
        "    \n",
        "    if not mean:\n",
        "        # error handling in case mean cannot be calculated\n",
        "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
        "        # FIXME: remove these examples in pre-processing\n",
        "        return np.zeros(wv.vector_size,)\n",
        "\n",
        "    # use gensim's method to calculate the mean of all the words appended to mean list\n",
        "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
        "    return mean\n",
        "\n",
        "def  word_averaging_list(wv, text_list):\n",
        "    return np.vstack([word_averaging(wv, post) for post in text_list ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tm_febK-xoAC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Below, we explore a different way to create tokens out of sentences, by using the nltk toolkit. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jlyXtYm1QIos",
        "colab": {},
        "outputId": "07fff7ce-20dd-4fa8-d75f-28591743e455"
      },
      "cell_type": "code",
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "def w2v_tokenize_text(text):\n",
        "    # create tokens, a list of words, for each post. This function will do some cleaning based on English language\n",
        "    tokens = []\n",
        "    for sent in nltk.sent_tokenize(text, language='english'):\n",
        "        for word in nltk.word_tokenize(sent, language='english'):\n",
        "            if len(word) < 2:\n",
        "                continue\n",
        "            tokens.append(word)\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/dorien/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZeuQn-3GxoAQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's split the dataset in training and test set like before, and tokenize each of the datasets"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "CA1Wrn9-QIot",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
        "\n",
        "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values\n",
        "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['post']), axis=1).values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rh11CM3ZxoAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can then average the position per post in this new dataset using the functions we defined above and based on our word2vec model wv."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "pqG34rU6QIoy",
        "colab": {},
        "outputId": "0c75137f-efca-40f1-ec26-15eeec4e065d"
      },
      "cell_type": "code",
      "source": [
        "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
        "X_test_word_average = word_averaging_list(wv,test_tokenized)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/Cellar/python3/3.6.0/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
            "  if sys.path[0] == '':\n",
            "WARNING:root:cannot compute similarity with no input []\n",
            "WARNING:root:cannot compute similarity with no input ['ngrepeat']\n",
            "WARNING:root:cannot compute similarity with no input []\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "3zQa7btkxoAj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we can feed this new representation into the logistic regression: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "LSSSuFQYQIo5",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg = logreg.fit(X_train_word_average, train['tags'])\n",
        "y_pred = logreg.predict(X_test_word_average)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "U_WzSihIQIo9",
        "colab": {},
        "outputId": "cc9d6ecd-478c-4dae-ab6a-1a9f99a4647e"
      },
      "cell_type": "code",
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
        "print(classification_report(test.tags, y_pred,target_names=my_tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.637416666667\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.62      0.59      0.61       613\n",
            "         html       0.74      0.76      0.75       620\n",
            "      asp.net       0.65      0.67      0.66       587\n",
            "           c#       0.53      0.52      0.52       586\n",
            "ruby-on-rails       0.70      0.77      0.73       599\n",
            "       jquery       0.44      0.39      0.41       589\n",
            "        mysql       0.65      0.60      0.63       594\n",
            "          php       0.73      0.80      0.76       610\n",
            "          ios       0.60      0.61      0.60       617\n",
            "   javascript       0.56      0.52      0.54       587\n",
            "       python       0.55      0.50      0.52       611\n",
            "            c       0.61      0.61      0.61       594\n",
            "          css       0.65      0.65      0.65       619\n",
            "      android       0.61      0.57      0.59       574\n",
            "       iphone       0.70      0.71      0.71       584\n",
            "          sql       0.42      0.43      0.42       578\n",
            "  objective-c       0.68      0.70      0.69       591\n",
            "          c++       0.76      0.78      0.77       608\n",
            "    angularjs       0.82      0.83      0.82       638\n",
            "         .net       0.65      0.71      0.68       601\n",
            "\n",
            "  avg / total       0.63      0.64      0.64     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cNXuL03bxoAy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now you can see that the accuracy went down! Oh no! Why is that? Because we used a very naive approach, a BOW to average our vectors. The way around it would be doc2vec, which learns relationships between documents (posts in this case), instead of words.  "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "qtujiFgzQIpA"
      },
      "cell_type": "markdown",
      "source": [
        "### Doc2vec and Logistic Regression (advanced, optional)\n",
        "\n",
        "The idea of word2vec can be extended to documents where instead of learning feature representations for words, we learn it for sentences or documents. To get a general idea of a word2vec, think of it as a mathematical average of the word vector representations of all the words in the document. Doc2Vec extends the idea of word2vec, however words can only capture so much, there are times when we need relationships between documents and not just words.\n",
        "\n",
        "The way to train doc2vec model for our Stack Overflow questions and tags data is very similar with when we train Multi-Class Text Classification with Doc2vec and Logistic Regression.\n",
        "\n",
        "First, we label the sentences. Gensims Doc2Vec implementation requires each document/paragraph to have a label associated with it. We do this by using the TaggedDocument method. The format will be TRAIN_i or TEST_i where i is a dummy index of the post.\n",
        "\n",
        "First let's import the necessary libraries. \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qkXdv0A6QIpB",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tqdm.pandas(desc=\"progress-bar\")\n",
        "from gensim.models import Doc2Vec\n",
        "from sklearn import utils\n",
        "import gensim\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "import re\n",
        "from gensim.models import doc2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cBov76MXxoA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's start by defining a function that labels our sentences in the corpus. We just give them dummy labels TRAIN_i or TEST_i for post i. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "TtVzwM8RQIpD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def label_sentences(corpus, label_type):\n",
        "    \"\"\"\n",
        "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
        "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
        "    a dummy index of the post.\n",
        "    \"\"\"\n",
        "    labeled = []\n",
        "    for i, v in enumerate(corpus):\n",
        "        label = label_type + '_' + str(i)\n",
        "        labeled.append(doc2vec.TaggedDocument(v.split(), [label]))\n",
        "    return labeled"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5lsDfgBnxoBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Just like above we split our dataset up in test and training data."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GfXghWoJQIpF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.post, df.tags, random_state=0, test_size=0.3)\n",
        "X_train = label_sentences(X_train, 'Train')\n",
        "X_test = label_sentences(X_test, 'Test')\n",
        "all_data = X_train + X_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_KYhbHmxoBI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's have a look how our data looks at this moment: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "321apZFWQIpI",
        "colab": {},
        "outputId": "bfb8d0d7-eb03-4671-adc9-0565027bf3d7"
      },
      "cell_type": "code",
      "source": [
        "all_data[:2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TaggedDocument(words=['fulltext', 'search', 'php', 'pdo', 'returning', 'result', 'searched', 'lot', 'matter', 'find', 'wrong', 'setup', 'trying', 'fulltext', 'search', 'using', 'pdo', 'php', 'get', 'results', 'error', 'messages', 'table', 'contains', 'customer', 'details', 'id', 'int', '11', 'auto_increment', 'name', 'varchar', '150', 'lastname', 'varchar', '150', 'company', 'varchar', '250', 'adress', 'varchar', '150', 'postcode', 'int', '5', 'city', 'varchar', '150', 'email', 'varchar', '250', 'phone', 'varchar', '20', 'orgnr', 'varchar', '15', 'timestamp', 'timestamp', 'current_timestamp', 'run', 'sqlquery', 'alter', 'table', 'system_customer', 'add', 'fulltext', 'name', 'lastname', 'except', 'columns', 'id', 'postcode', 'timestamp', 'signs', 'trouble', 'far', 'idea', 'problem', 'lies', 'db', 'configuration', 'php', 'code', 'goes', 'php', 'sth', 'dbhprepare', 'select', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'db_pre', 'customer', 'match', 'name', 'lastname', 'company', 'adress', 'city', 'phone', 'email', 'orgnr', 'search', 'boolean', 'mode', 'bind', 'placeholders', 'sthbindparam', 'search', 'data', 'sthexecute', 'rows', 'sthfetchall', 'testing', 'print_r', 'dbherrorinfo', 'empty', 'rows', 'echo', 'else', 'echo', 'foreach', 'rows', 'row', 'echo', 'tr', 'datahref', 'new_orderphp', 'cid', 'row', 'id', 'echo', 'td', 'row', 'name', 'td', 'echo', 'td', 'row', 'lastname', 'td', 'echo', 'td', 'row', 'company', 'td', 'echo', 'td', 'row', 'phone', 'td', 'echo', 'td', 'row', 'email', 'td', 'echo', 'td', 'date', 'ymd', 'strtotime', 'row', 'timestamp', 'td', 'echo', 'tr', 'echo', 'tried', 'change', 'parameter', 'searchquery', 'string', 'like', 'testcompany', 'somename', 'boolean', 'mode', 'also', 'read', 'word', 'found', '50', 'rows', 'counts', 'common', 'word', 'pretty', 'sure', 'case', 'uses', 'specific', 'words', 'table', 'uses', 'myisam', 'engine', 'get', 'results', 'error', 'messages', 'please', 'help', 'point', 'wrong', 'thank'], tags=['Train_0']),\n",
              " TaggedDocument(words=['select', 'everything', '1', 'table', 'x', 'rows', 'another', 'im', 'making', 'join', 'query', 'like', 'select', 'clothes', 'c', 'join', 'style', 'cstyleid', 'ssylelid', 'clothesid', '19', 'dont', 'want', 'select', 'everything', 'style', 'want', 'select', 'everything', 'clothes', '20', 'rows', 'select', '1', 'row', '10', 'style', 'easyest', 'way', 'without', 'select', 'every', 'row', 'clothes', '20', 'things', 'select', 'like', 'select', 'cid', 'cdescription', 'cname', 'csize', 'cbrand', 'sname', 'clothes', 'c', 'join', 'style', 'cstyleid', 'stsylelid', 'clothesid', '19', 'would', 'fastest', 'way', 'possibillity'], tags=['Train_1'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "metadata": {
        "id": "GvY-IsHNxoBQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gensim allows us to build a model very easily. We can vary the parameters to fit your data: \n",
        "\n",
        "*    dm=0 , distributed bag of words (DBOW) is used.\n",
        "*    vector_size=300 , 300 vector dimensional feature vectors.\n",
        "*    negative=5 , specifies how many noise words should be drawn.\n",
        "*    min_count=1, ignores all words with total frequency lower than this.\n",
        "*    alpha=0.065 , the initial learning rate.\n",
        "\n",
        "We initialize the model and train for 30 epochs. (slower computers may want to train for less epochs)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "G9UoqpKnQIpM",
        "colab": {},
        "outputId": "feb3ff93-2450-4f02-b952-472aa7839ef9"
      },
      "cell_type": "code",
      "source": [
        "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
        "model_dbow.build_vocab([x for x in tqdm(all_data)])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 40000/40000 [00:00<00:00, 1448221.88it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lSDy4huyQIpP",
        "colab": {},
        "outputId": "4d6f9545-5217-436a-8a48-6d261a828047"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(30):\n",
        "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
        "    model_dbow.alpha -= 0.002\n",
        "    model_dbow.min_alpha = model_dbow.alpha"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 40000/40000 [00:00<00:00, 1668395.27it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2076516.62it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1294578.23it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2081385.50it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1935445.53it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2106922.85it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2088458.79it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2056661.48it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1985516.34it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1942234.52it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2058453.08it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2120343.25it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2091791.78it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2397498.64it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2386619.06it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2094690.74it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1960688.11it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1680326.11it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1468645.26it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1030155.53it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1541725.95it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2374996.96it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2303613.35it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1596613.63it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1694702.52it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 2081282.22it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1566470.84it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1695062.09it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1836063.74it/s]\n",
            "100%|| 40000/40000 [00:00<00:00, 1987680.50it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "mNp4a3ouxoB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now let's get the vectors out of this trained  model so that we can feed them into the logistic regression:"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A6cGBFuRQIpT",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
        "    \"\"\"\n",
        "    Get vectors from trained doc2vec model\n",
        "    :param doc2vec_model: Trained Doc2Vec model\n",
        "    :param corpus_size: Size of the data\n",
        "    :param vectors_size: Size of the embedding vectors\n",
        "    :param vectors_type: Training or Testing vectors\n",
        "    :return: list of vectors\n",
        "    \"\"\"\n",
        "    vectors = np.zeros((corpus_size, vectors_size))\n",
        "    for i in range(0, corpus_size):\n",
        "        prefix = vectors_type + '_' + str(i)\n",
        "        vectors[i] = model.docvecs[prefix]\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "57wLrRE3QIpX",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
        "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WJE2ZeuOxoCD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now feed these vectors to the classifier again: "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "NoLo0XkeQIpa",
        "colab": {},
        "outputId": "3142c993-74b5-429a-806e-9cc6f4e90bc5"
      },
      "cell_type": "code",
      "source": [
        "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
        "logreg.fit(train_vectors_dbow, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
              "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
              "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
              "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AT6_TZgAQIpd",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
        "y_pred = logreg.predict(test_vectors_dbow)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "UfyHgX3KQIpg",
        "colab": {},
        "outputId": "0103b21f-adf9-44b6-ef01-39a0d249bc83"
      },
      "cell_type": "code",
      "source": [
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print(classification_report(y_test, y_pred,target_names=my_tags))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy 0.80525\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         java       0.71      0.66      0.68       589\n",
            "         html       0.89      0.91      0.90       661\n",
            "      asp.net       0.93      0.94      0.93       606\n",
            "           c#       0.78      0.78      0.78       613\n",
            "ruby-on-rails       0.84      0.90      0.87       601\n",
            "       jquery       0.72      0.72      0.72       585\n",
            "        mysql       0.87      0.82      0.84       621\n",
            "          php       0.81      0.87      0.84       587\n",
            "          ios       0.72      0.67      0.69       560\n",
            "   javascript       0.66      0.65      0.66       611\n",
            "       python       0.66      0.67      0.66       593\n",
            "            c       0.81      0.83      0.82       581\n",
            "          css       0.80      0.77      0.78       608\n",
            "      android       0.84      0.85      0.85       593\n",
            "       iphone       0.85      0.81      0.83       592\n",
            "          sql       0.70      0.64      0.67       597\n",
            "  objective-c       0.83      0.87      0.85       604\n",
            "          c++       0.91      0.94      0.93       610\n",
            "    angularjs       0.93      0.95      0.94       595\n",
            "         .net       0.80      0.83      0.82       593\n",
            "\n",
            "  avg / total       0.80      0.81      0.80     12000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GqmumA0-xoCV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "80%, that is the best result so far! Remember, we can actually use any classifier with this method! So up to you to make your project as efficient as possible :)\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "jvthUI_yxoCg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "* https://radimrehurek.com/gensim/models/word2vec.html\n",
        "* https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
        "* https://github.com/kavgan/nlp-text-mining-working-examples/tree/master/word2vec\n",
        "* https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5"
      ]
    },
    {
      "metadata": {
        "id": "6_oThGFVxoCh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}